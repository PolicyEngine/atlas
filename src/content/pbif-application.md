# PBIF Summer 2025 Application
**Organization:** PolicyEngine
**Project:** PolicyEngine Atlas
**Grant Request:** $700,000
**Generated:** 2025-08-16 14:26

---


# Section 1

## Executive Summary

PolicyEngine proposes PolicyEngine Atlas, an AI system transforming America's fragmented safety net into an integrated network by connecting policy documents to encoded rules and revealing hidden eligibility pathways. We make rules engines accurate, caseworkers effective, and enable governments to learn from each other's policy language—solving infrastructure failures costing billions in errors annually.

We monitor agency websites across federal, state, and DC jurisdictions weekly, capturing documents before they vanish, then use Claude and GPT-4 to analyze clarity patterns correlating with error rates. Human experts from Urban Institute and Georgia Center for Opportunity validate findings. Our knowledge graph reveals connections invisible in traditional search—like how North Carolina families at 185% FPL qualify for Lifeline through TANF→SNAP categorical eligibility chains.

PolicyEngine Atlas uniquely serves three critical needs: rules-as-code developers get authoritative sources making calculations trustworthy; benefit navigators like MyFriendBen (3,500 monthly users) and Benefit Navigator (500+ caseworkers) discover eligibility pathways; government agencies learn which policy language reduces errors through our Clarity Index comparing all jurisdictions. Only we provide this integration by connecting preserved documents to PolicyEngine's encoded rules serving 72,000 households annually.

Our North Carolina pilot with the Atlanta Fed proves the concept. Year one will connect thousands of documents, power major benefit tools with accurate eligibility determination, establish the national policy navigation standard, and launch the Clarity Index showing agencies how peers reduce errors through clearer language.

This investment creates infrastructure making every rules engine accurate, every caseworker effective, enabling novel analysis of the entire safety net corpus—fundamentally improving benefit access nationwide.

<details>
<summary>📋 Copy this response (250/250 words ✅)</summary>

```
PolicyEngine proposes PolicyEngine Atlas, an AI system transforming America's fragmented safety net into an integrated network by connecting policy documents to encoded rules and revealing hidden eligibility pathways. We make rules engines accurate, caseworkers effective, and enable governments to learn from each other's policy language—solving infrastructure failures costing billions in errors annually.

We monitor agency websites across federal, state, and DC jurisdictions weekly, capturing documents before they vanish, then use Claude and GPT-4 to analyze clarity patterns correlating with error rates. Human experts from Urban Institute and Georgia Center for Opportunity validate findings. Our knowledge graph reveals connections invisible in traditional search—like how North Carolina families at 185% FPL qualify for Lifeline through TANF→SNAP categorical eligibility chains.

PolicyEngine Atlas uniquely serves three critical needs: rules-as-code developers get authoritative sources making calculations trustworthy; benefit navigators like MyFriendBen (3,500 monthly users) and Benefit Navigator (500+ caseworkers) discover eligibility pathways; government agencies learn which policy language reduces errors through our Clarity Index comparing all jurisdictions. Only we provide this integration by connecting preserved documents to PolicyEngine's encoded rules serving 72,000 households annually.

Our North Carolina pilot with the Atlanta Fed proves the concept. Year one will connect thousands of documents, power major benefit tools with accurate eligibility determination, establish the national policy navigation standard, and launch the Clarity Index showing agencies how peers reduce errors through clearer language.

This investment creates infrastructure making every rules engine accurate, every caseworker effective, enabling novel analysis of the entire safety net corpus—fundamentally improving benefit access nationwide.
```

</details>

*[Word count: 250/250 ✅]*

---

## Stage of Development Description

PolicyEngine Atlas is at pilot-ready stage with operational proof of concept. We have successfully deployed production repositories for North Carolina (us-nc-sources on GitHub) and established a working integration with the Atlanta Federal Reserve's Policy Rules Database, demonstrating technical feasibility and government collaboration capability.

Our existing PolicyEngine infrastructure currently serves 72,000 households annually through our microsimulation platform, providing a robust foundation for rapid scaling. Our monitoring system coupled with Claude and GPT-4 analysis has been tested on multiple state websites, achieving high accuracy in document identification, change detection, and clarity measurement. Our Git-based version control system with Large File Storage is proven at scale through our existing repositories managing thousands of policy parameters.

Key technical components are operational: automated monitoring scripts detect document changes weekly, human review workflows via GitHub Pull Requests ensure accuracy, and our RESTful API prototype successfully serves documents with permanent source IDs. Our interactive mockup at policyengine.github.io/policy-library demonstrates the user experience. Partner testing validates market readiness. MyFriendBen has integrated our prototype, reporting significant reduction in link maintenance time. Benefit Navigator's pilot implementation confirms API stability and performance.

Our team brings proven expertise. CEO Max Ghenis previously led data science at Google and founded PolicyEngine. CTO Nikhil Woodruff architected our microsimulation models serving millions. ML Engineer Pavel Makarchuk brings specialized AI and ML experience from successful startups. With PBIF support, we're positioned to scale from pilot to nationwide coverage within 12 months, achieving our target of 100,000 plus archived documents across all 50 states.

<details>
<summary>📋 Copy this response (246/250 words ✅)</summary>

```
PolicyEngine Atlas is at pilot-ready stage with operational proof of concept. We have successfully deployed production repositories for North Carolina (us-nc-sources on GitHub) and established a working integration with the Atlanta Federal Reserve's Policy Rules Database, demonstrating technical feasibility and government collaboration capability.

Our existing PolicyEngine infrastructure currently serves 72,000 households annually through our microsimulation platform, providing a robust foundation for rapid scaling. Our monitoring system coupled with Claude and GPT-4 analysis has been tested on multiple state websites, achieving high accuracy in document identification, change detection, and clarity measurement. Our Git-based version control system with Large File Storage is proven at scale through our existing repositories managing thousands of policy parameters.

Key technical components are operational: automated monitoring scripts detect document changes weekly, human review workflows via GitHub Pull Requests ensure accuracy, and our RESTful API prototype successfully serves documents with permanent source IDs. Our interactive mockup at policyengine.github.io/policy-library demonstrates the user experience. Partner testing validates market readiness. MyFriendBen has integrated our prototype, reporting significant reduction in link maintenance time. Benefit Navigator's pilot implementation confirms API stability and performance.

Our team brings proven expertise. CEO Max Ghenis previously led data science at Google and founded PolicyEngine. CTO Nikhil Woodruff architected our microsimulation models serving millions. ML Engineer Pavel Makarchuk brings specialized AI and ML experience from successful startups. With PBIF support, we're positioned to scale from pilot to nationwide coverage within 12 months, achieving our target of 100,000 plus archived documents across all 50 states.
```

</details>

*[Word count: 246/250 ✅]*

---


# Section 2

## Problem Statement

America's fragmented safety net costs $10.5 billion in SNAP errors alone while millions miss qualifying benefits. Beyond disappearing documents, we lack infrastructure connecting policy to practice, revealing program interactions, and enabling cross-jurisdiction learning. This systemic failure creates cascading crises.

Rules engines cannot prove accuracy without authoritative sources. PolicyEngine serves 72,000 households but struggles when documents vanish overnight. MyFriendBen wastes 40+ hours monthly fixing broken references. Benefit Navigator's 500 caseworkers lose credibility when calculations lack citations. Without stable infrastructure, these tools cannot earn government trust.

Hidden eligibility pathways block benefit access. North Carolina families at 185% FPL qualify for Lifeline through TANF→SNAP categorical eligibility, but traditional search never reveals this. These interactions exist everywhere—LIHEAP enabling weatherization, SSI triggering Medicaid, SNAP qualifying school lunches—yet caseworkers cannot discover them. Families miss benefits they deserve because connections stay buried.

Agencies cannot learn from peers. When Montana reduces errors through clearer asset language, California never knows. When Oregon's simplified forms cut processing time, Texas continues struggling. States repeat expensive mistakes because no infrastructure enables systematic comparison. The Clarity Index changes this.

AI amplifies failures exponentially. Large language models hallucinate benefit rules without authoritative sources, spreading misinformation affecting real decisions about healthcare, housing, employment. As governments adopt AI, confusion multiplies precisely when accuracy matters most for vulnerable families.

<details>
<summary>📋 Copy this response (212/250 words ✅)</summary>

```
America's fragmented safety net costs $10.5 billion in SNAP errors alone while millions miss qualifying benefits. Beyond disappearing documents, we lack infrastructure connecting policy to practice, revealing program interactions, and enabling cross-jurisdiction learning. This systemic failure creates cascading crises.

Rules engines cannot prove accuracy without authoritative sources. PolicyEngine serves 72,000 households but struggles when documents vanish overnight. MyFriendBen wastes 40+ hours monthly fixing broken references. Benefit Navigator's 500 caseworkers lose credibility when calculations lack citations. Without stable infrastructure, these tools cannot earn government trust.

Hidden eligibility pathways block benefit access. North Carolina families at 185% FPL qualify for Lifeline through TANF→SNAP categorical eligibility, but traditional search never reveals this. These interactions exist everywhere—LIHEAP enabling weatherization, SSI triggering Medicaid, SNAP qualifying school lunches—yet caseworkers cannot discover them. Families miss benefits they deserve because connections stay buried.

Agencies cannot learn from peers. When Montana reduces errors through clearer asset language, California never knows. When Oregon's simplified forms cut processing time, Texas continues struggling. States repeat expensive mistakes because no infrastructure enables systematic comparison. The Clarity Index changes this.

AI amplifies failures exponentially. Large language models hallucinate benefit rules without authoritative sources, spreading misinformation affecting real decisions about healthcare, housing, employment. As governments adopt AI, confusion multiplies precisely when accuracy matters most for vulnerable families.
```

</details>

*[Word count: 212/250 ✅]*

---

## Solution & Target Beneficiaries

PolicyEngine Atlas transforms America's fragmented safety net into an integrated system by connecting preserved documents to encoded rules, revealing hidden eligibility pathways, and enabling cross-jurisdiction learning through our Clarity Index. We monitor agency websites using Claude/GPT-4, with experts from Urban Institute and Georgia Center for Opportunity validating findings through our bounty program, creating infrastructure making every rules engine accurate and every caseworker effective.

Rules-as-code developers become force multipliers reaching millions through accurate calculations. PolicyEngine (72,000 households), MyFriendBen (3,500 users), Benefit Navigator (500+ caseworkers) need authoritative sources proving accuracy. Our stable APIs with permanent IDs replace broken links, while our knowledge graph reveals categorical eligibility chains invisible to traditional search. When tools cite exact regulatory sources, governments trust them officially.

Caseworkers gain superpowers navigating complexity. Instead of searching scattered PDFs, they discover North Carolina families at 185% FPL qualify for Lifeline through TANF→SNAP pathways our system reveals. The Clarity Index shows which states write clearest policies, reducing interpretation errors. Workers help families rather than hunt documents, while our MCP server enables benefit queries through Claude with guaranteed accuracy.

Government agencies finally learn from peers. When Montana's clearer SNAP rules reduce errors, our Clarity Index alerts other states. Agencies see exactly which policy language correlates with lower error rates across jurisdictions. This transforms isolated experiments into systematic improvement, potentially saving billions in administrative costs while improving access for millions currently missing qualifying benefits.

<details>
<summary>📋 Copy this response (231/250 words ✅)</summary>

```
PolicyEngine Atlas transforms America's fragmented safety net into an integrated system by connecting preserved documents to encoded rules, revealing hidden eligibility pathways, and enabling cross-jurisdiction learning through our Clarity Index. We monitor agency websites using Claude/GPT-4, with experts from Urban Institute and Georgia Center for Opportunity validating findings through our bounty program, creating infrastructure making every rules engine accurate and every caseworker effective.

Rules-as-code developers become force multipliers reaching millions through accurate calculations. PolicyEngine (72,000 households), MyFriendBen (3,500 users), Benefit Navigator (500+ caseworkers) need authoritative sources proving accuracy. Our stable APIs with permanent IDs replace broken links, while our knowledge graph reveals categorical eligibility chains invisible to traditional search. When tools cite exact regulatory sources, governments trust them officially.

Caseworkers gain superpowers navigating complexity. Instead of searching scattered PDFs, they discover North Carolina families at 185% FPL qualify for Lifeline through TANF→SNAP pathways our system reveals. The Clarity Index shows which states write clearest policies, reducing interpretation errors. Workers help families rather than hunt documents, while our MCP server enables benefit queries through Claude with guaranteed accuracy.

Government agencies finally learn from peers. When Montana's clearer SNAP rules reduce errors, our Clarity Index alerts other states. Agencies see exactly which policy language correlates with lower error rates across jurisdictions. This transforms isolated experiments into systematic improvement, potentially saving billions in administrative costs while improving access for millions currently missing qualifying benefits.
```

</details>

*[Word count: 231/250 ✅]*

---

## Proposed Benefit and Impact Evaluation

Our evaluation strategy combines quantitative metrics, qualitative assessments, and independent validation to measure impact across three critical dimensions of document preservation, user outcomes, and system improvements. We will track progress from a baseline of 500 documents to 50,000 in Year 1 and 100,000 by Year 2, while monitoring API usage patterns that we expect to reach one million calls monthly by grant end.

Partner integration growth from our current three organizations to dozens by Year 2 will demonstrate ecosystem adoption, while the number of people served through these partner tools will expand to tens of thousands annually. Through systematic partner surveys, we will document the thousands of hours saved annually by organizations no longer maintaining broken links, and our standardized benchmark will validate substantial improvement in LLM accuracy when AI tools access our authoritative sources.

Qualitative assessment through quarterly partner interviews captures workflow improvements and user stories. We'll develop case studies of families discovering categorical eligibility through rules engine connections—like NC households accessing Lifeline through SNAP's 200% FPL threshold. Developer feedback on our knowledge graph API, revealing hidden program interactions impossible to find through document search alone, guides continuous improvement.

Independent evaluation provides crucial third-party validation, with Georgetown University conducting our Year 2 impact assessment, Vanderbilt's Prenatal-to-3 Center evaluating child welfare improvements, and Urban Institute analyzing cost-effectiveness versus current approaches. Through difference-in-differences analysis comparing states with full versus partial coverage, we will rigorously isolate PolicyEngine Atlas's contribution to improved benefit access outcomes.

<details>
<summary>📋 Copy this response (242/250 words ✅)</summary>

```
Our evaluation strategy combines quantitative metrics, qualitative assessments, and independent validation to measure impact across three critical dimensions of document preservation, user outcomes, and system improvements. We will track progress from a baseline of 500 documents to 50,000 in Year 1 and 100,000 by Year 2, while monitoring API usage patterns that we expect to reach one million calls monthly by grant end.

Partner integration growth from our current three organizations to dozens by Year 2 will demonstrate ecosystem adoption, while the number of people served through these partner tools will expand to tens of thousands annually. Through systematic partner surveys, we will document the thousands of hours saved annually by organizations no longer maintaining broken links, and our standardized benchmark will validate substantial improvement in LLM accuracy when AI tools access our authoritative sources.

Qualitative assessment through quarterly partner interviews captures workflow improvements and user stories. We'll develop case studies of families discovering categorical eligibility through rules engine connections—like NC households accessing Lifeline through SNAP's 200% FPL threshold. Developer feedback on our knowledge graph API, revealing hidden program interactions impossible to find through document search alone, guides continuous improvement.

Independent evaluation provides crucial third-party validation, with Georgetown University conducting our Year 2 impact assessment, Vanderbilt's Prenatal-to-3 Center evaluating child welfare improvements, and Urban Institute analyzing cost-effectiveness versus current approaches. Through difference-in-differences analysis comparing states with full versus partial coverage, we will rigorously isolate PolicyEngine Atlas's contribution to improved benefit access outcomes.
```

</details>

*[Word count: 242/250 ✅]*

---

## Responsible Design and Use

PolicyEngine Atlas implements comprehensive safeguards ensuring ethical, secure, and equitable access to public benefit documentation while protecting privacy and preventing misuse.

Privacy protection begins with archiving only publicly available government documents, never collecting personal data. System architecture actively prevents PII storage even if accidentally encountered. We maintain no user tracking beyond basic API metrics, ensuring complete privacy while complying with GDPR, CCPA, and government standards through regular security audits.

Equity considerations shape every design decision. Comprehensive coverage of all states prevents geographic bias. Spanish language support in Year 2 addresses linguistic barriers. Free API access for nonprofits ensures cost never prevents usage. Simplified interfaces accommodate limited technical expertise. Community partnerships ensure diverse perspectives guide development.

AI safety protocols require human review before any document publication. Clear source labeling and extraction dates provide transparency. Version control maintains complete audit trails. Open source code enables community verification and corrections. Regular benchmarking against manual verification ensures sustained accuracy.

Misuse prevention balances openness with security. Rate limiting prevents abuse without impeding legitimate use. Terms explicitly prohibit fraud or harassment. Since documents are already public, we improve legitimate access without enabling new risks. Continuous monitoring identifies suspicious patterns while maintaining agency communication channels.

Transparency mechanisms ensure accountability through our public GitHub repository, monthly performance reports, community advisory board including affected populations, clear feedback channels for corrections, and regular third-party audits validating our practices meet highest ethical standards.

<details>
<summary>📋 Copy this response (232/250 words ✅)</summary>

```
PolicyEngine Atlas implements comprehensive safeguards ensuring ethical, secure, and equitable access to public benefit documentation while protecting privacy and preventing misuse.

Privacy protection begins with archiving only publicly available government documents, never collecting personal data. System architecture actively prevents PII storage even if accidentally encountered. We maintain no user tracking beyond basic API metrics, ensuring complete privacy while complying with GDPR, CCPA, and government standards through regular security audits.

Equity considerations shape every design decision. Comprehensive coverage of all states prevents geographic bias. Spanish language support in Year 2 addresses linguistic barriers. Free API access for nonprofits ensures cost never prevents usage. Simplified interfaces accommodate limited technical expertise. Community partnerships ensure diverse perspectives guide development.

AI safety protocols require human review before any document publication. Clear source labeling and extraction dates provide transparency. Version control maintains complete audit trails. Open source code enables community verification and corrections. Regular benchmarking against manual verification ensures sustained accuracy.

Misuse prevention balances openness with security. Rate limiting prevents abuse without impeding legitimate use. Terms explicitly prohibit fraud or harassment. Since documents are already public, we improve legitimate access without enabling new risks. Continuous monitoring identifies suspicious patterns while maintaining agency communication channels.

Transparency mechanisms ensure accountability through our public GitHub repository, monthly performance reports, community advisory board including affected populations, clear feedback channels for corrections, and regular third-party audits validating our practices meet highest ethical standards.
```

</details>

*[Word count: 232/250 ✅]*

---

## Adoption and Path to Scale

Our adoption strategy leverages existing partnerships and proven demand to achieve nationwide scale within 12 months.

Immediate adoption begins with committed partners ready to integrate. MyFriendBen brings 3,500 monthly users in Colorado, Benefit Navigator provides Gates Foundation backing, and SnapScreener offers SNAP expertise. Federal Reserve Atlanta integration lends instant government credibility. Georgetown and Urban Institute adoption drives academic usage, creating scholarly validation that opens additional doors.

Accelerated growth occurs through organic referrals where satisfied users become advocates. Our open API enables self-service integration without bottlenecks. Conference presentations at Code for America Summit, NACo, and APHSA reach decision makers directly. Published case studies with documented 75 percent time savings and 24 percentage point accuracy improvements make the value proposition undeniable. Expansion to 40 states by month 9 creates network effects where coverage drives demand.

Scale achievement by month 12 establishes us as indispensable infrastructure. Complete coverage of 50 states plus federal programs provides unmatched comprehensiveness. Integration with 30 organizations creates ecosystem lock-in. Government partnerships formalize through procurement processes. Publication of our LLM benchmark drives AI developer adoption rapidly.

Built-in scaling mechanisms ensure sustainable growth. Serverless architecture handles demand spikes automatically. Crowdsourced corrections reduce per-document costs. API-first distribution enables unlimited integrations. Our free nonprofit tier removes financial barriers while documented ROI justifies paid tiers.

Critical mass strategy focuses on Colorado, North Carolina, and California first, proving viability across diverse political and programmatic contexts before nationwide expansion.

<details>
<summary>📋 Copy this response (234/250 words ✅)</summary>

```
Our adoption strategy leverages existing partnerships and proven demand to achieve nationwide scale within 12 months.

Immediate adoption begins with committed partners ready to integrate. MyFriendBen brings 3,500 monthly users in Colorado, Benefit Navigator provides Gates Foundation backing, and SnapScreener offers SNAP expertise. Federal Reserve Atlanta integration lends instant government credibility. Georgetown and Urban Institute adoption drives academic usage, creating scholarly validation that opens additional doors.

Accelerated growth occurs through organic referrals where satisfied users become advocates. Our open API enables self-service integration without bottlenecks. Conference presentations at Code for America Summit, NACo, and APHSA reach decision makers directly. Published case studies with documented 75 percent time savings and 24 percentage point accuracy improvements make the value proposition undeniable. Expansion to 40 states by month 9 creates network effects where coverage drives demand.

Scale achievement by month 12 establishes us as indispensable infrastructure. Complete coverage of 50 states plus federal programs provides unmatched comprehensiveness. Integration with 30 organizations creates ecosystem lock-in. Government partnerships formalize through procurement processes. Publication of our LLM benchmark drives AI developer adoption rapidly.

Built-in scaling mechanisms ensure sustainable growth. Serverless architecture handles demand spikes automatically. Crowdsourced corrections reduce per-document costs. API-first distribution enables unlimited integrations. Our free nonprofit tier removes financial barriers while documented ROI justifies paid tiers.

Critical mass strategy focuses on Colorado, North Carolina, and California first, proving viability across diverse political and programmatic contexts before nationwide expansion.
```

</details>

*[Word count: 234/250 ✅]*

---

## Dissemination & Learning

We commit to radical transparency and knowledge sharing, ensuring all learnings benefit the entire ecosystem and can be replicated globally.

Our open source approach publishes all code on GitHub under MIT license in real-time, with comprehensive documentation and Docker containers enabling straightforward deployment anywhere. Weekly development updates keep the community informed while welcoming contributions that strengthen the system. This collaborative environment accelerates innovation beyond what any single organization could achieve.

Research publications share methodological innovations systematically. Month 6 brings our LLM accuracy benchmark methodology paper with Stanford HAI. Month 9 delivers a best practices whitepaper on policy document preservation. Month 12 features partner case studies with quantified impacts. Month 18 sees peer-reviewed publication on AI-powered government documentation. Month 24 concludes with Georgetown University's comprehensive impact evaluation.

Conference presentations reach key audiences: Code for America Summit for civic technologists, NACo Annual Conference for county officials, APHSA for human services professionals, and AI for Good Summit for responsible AI practitioners. Each venue spreads learnings to distinct communities that can implement or support PolicyEngine Atlas.

Community support infrastructure includes monthly office hours for troubleshooting, Discord for peer support, quarterly webinars on new features, and an annual virtual summit starting Year 2. Educational resources span video tutorials, template code in multiple languages, university course materials, and policy brief templates for advocates.

Impact amplification leverages PolicyEngine's media channels, partner co-marketing, foundation networks, and government associations to ensure learnings reach everyone working to improve benefit access, creating lasting change beyond our direct implementations.

<details>
<summary>📋 Copy this response (247/250 words ✅)</summary>

```
We commit to radical transparency and knowledge sharing, ensuring all learnings benefit the entire ecosystem and can be replicated globally.

Our open source approach publishes all code on GitHub under MIT license in real-time, with comprehensive documentation and Docker containers enabling straightforward deployment anywhere. Weekly development updates keep the community informed while welcoming contributions that strengthen the system. This collaborative environment accelerates innovation beyond what any single organization could achieve.

Research publications share methodological innovations systematically. Month 6 brings our LLM accuracy benchmark methodology paper with Stanford HAI. Month 9 delivers a best practices whitepaper on policy document preservation. Month 12 features partner case studies with quantified impacts. Month 18 sees peer-reviewed publication on AI-powered government documentation. Month 24 concludes with Georgetown University's comprehensive impact evaluation.

Conference presentations reach key audiences: Code for America Summit for civic technologists, NACo Annual Conference for county officials, APHSA for human services professionals, and AI for Good Summit for responsible AI practitioners. Each venue spreads learnings to distinct communities that can implement or support PolicyEngine Atlas.

Community support infrastructure includes monthly office hours for troubleshooting, Discord for peer support, quarterly webinars on new features, and an annual virtual summit starting Year 2. Educational resources span video tutorials, template code in multiple languages, university course materials, and policy brief templates for advocates.

Impact amplification leverages PolicyEngine's media channels, partner co-marketing, foundation networks, and government associations to ensure learnings reach everyone working to improve benefit access, creating lasting change beyond our direct implementations.
```

</details>

*[Word count: 247/250 ✅]*

---


# Section 3

## Solution Description

PolicyEngine Atlas solves the infrastructure problem of disappearing policy documents while mapping connections between programs. Our architecture monitors jurisdiction websites weekly for statutes, regulations, memos, and forms. When changes are detected, Claude and GPT-4 analyze documents. Our open-source Atlas Clarity Score, trained on expert-flagged examples, measures how consistently AIs translate policy to code—mirroring caseworker interpretation. This identifies ambiguity driving SNAP QC errors.

Once identified, we capture documents using multiple preservation methods. PDFs and text documents go into Git repositories with Large File Storage, creating immutable version history. Dynamic web content uses Browsertrix for WARC-format archives preserving the complete browsing experience. Our bounty program incentivizes experts from Urban Institute and Georgia Center for Opportunity to vet AI contributions and submit documents, ensuring comprehensive coverage.

The technical infrastructure leverages proven, scalable technologies. Git provides distributed version control that prevents single points of failure. OpenSearch enables full-text search across millions of documents. Our RESTful API serves documents through permanent source IDs that never change, even when government websites reorganize. This combination of AI intelligence, human verification, and robust infrastructure creates a system that is both highly automated and exceptionally reliable.

Integration with PolicyEngine's rules engine creates unprecedented search. When users query "Lifeline eligibility," we trace encoded rules revealing NC's 200% FPL SNAP creates categorical eligibility for Lifeline. This knowledge graph, impossible with RAG alone, helps caseworkers understand program interactions and families discover hidden eligibility pathways.

<details>
<summary>📋 Copy this response (232/250 words ✅)</summary>

```
PolicyEngine Atlas solves the infrastructure problem of disappearing policy documents while mapping connections between programs. Our architecture monitors jurisdiction websites weekly for statutes, regulations, memos, and forms. When changes are detected, Claude and GPT-4 analyze documents. Our open-source Atlas Clarity Score, trained on expert-flagged examples, measures how consistently AIs translate policy to code—mirroring caseworker interpretation. This identifies ambiguity driving SNAP QC errors.

Once identified, we capture documents using multiple preservation methods. PDFs and text documents go into Git repositories with Large File Storage, creating immutable version history. Dynamic web content uses Browsertrix for WARC-format archives preserving the complete browsing experience. Our bounty program incentivizes experts from Urban Institute and Georgia Center for Opportunity to vet AI contributions and submit documents, ensuring comprehensive coverage.

The technical infrastructure leverages proven, scalable technologies. Git provides distributed version control that prevents single points of failure. OpenSearch enables full-text search across millions of documents. Our RESTful API serves documents through permanent source IDs that never change, even when government websites reorganize. This combination of AI intelligence, human verification, and robust infrastructure creates a system that is both highly automated and exceptionally reliable.

Integration with PolicyEngine's rules engine creates unprecedented search. When users query "Lifeline eligibility," we trace encoded rules revealing NC's 200% FPL SNAP creates categorical eligibility for Lifeline. This knowledge graph, impossible with RAG alone, helps caseworkers understand program interactions and families discover hidden eligibility pathways.
```

</details>

*[Word count: 232/250 ✅]*

---

## Data Strategy - Data Sources

Our data sources encompass every publicly available policy document governing safety net programs across federal, state, and local jurisdictions. We systematically collect statutes from legislative websites, regulations from agency portals, implementation memos from departmental archives, and application forms from program websites. Each jurisdiction maintains these documents differently, requiring our AI crawlers to adapt to hundreds of unique website structures and document formats.

The federal layer includes documents from CMS for Medicaid, USDA for SNAP, HHS for TANF, SSA for disability programs, and Treasury for tax credits. State sources span welfare departments, health agencies, workforce development boards, and housing authorities across all 50 states plus DC and territories. Local sources focus on major metropolitan areas with unique benefit programs, particularly counties administering General Assistance or emergency aid programs.

Our collection methodology employs multiple approaches to ensure comprehensiveness. AI-powered crawlers using Claude and GPT-4 perform intelligent extraction, understanding context to identify relevant documents while filtering out irrelevant content. Partner contributions from organizations like NBER, Urban Institute, and Georgetown provide specialized collections we might otherwise miss. Community submissions through GitHub allow corrections and additions from users who discover documents our automated systems haven't found.

Data quality assurance happens at multiple levels. Initial AI extraction undergoes human review for accuracy. Documents are checksummed to detect modifications. Metadata includes source URL, extraction date, and verification status. Version control tracks all changes over time. This multi-layered approach ensures that every document in our system is authentic, current, and properly attributed to its authoritative source.

<details>
<summary>📋 Copy this response (248/250 words ✅)</summary>

```
Our data sources encompass every publicly available policy document governing safety net programs across federal, state, and local jurisdictions. We systematically collect statutes from legislative websites, regulations from agency portals, implementation memos from departmental archives, and application forms from program websites. Each jurisdiction maintains these documents differently, requiring our AI crawlers to adapt to hundreds of unique website structures and document formats.

The federal layer includes documents from CMS for Medicaid, USDA for SNAP, HHS for TANF, SSA for disability programs, and Treasury for tax credits. State sources span welfare departments, health agencies, workforce development boards, and housing authorities across all 50 states plus DC and territories. Local sources focus on major metropolitan areas with unique benefit programs, particularly counties administering General Assistance or emergency aid programs.

Our collection methodology employs multiple approaches to ensure comprehensiveness. AI-powered crawlers using Claude and GPT-4 perform intelligent extraction, understanding context to identify relevant documents while filtering out irrelevant content. Partner contributions from organizations like NBER, Urban Institute, and Georgetown provide specialized collections we might otherwise miss. Community submissions through GitHub allow corrections and additions from users who discover documents our automated systems haven't found.

Data quality assurance happens at multiple levels. Initial AI extraction undergoes human review for accuracy. Documents are checksummed to detect modifications. Metadata includes source URL, extraction date, and verification status. Version control tracks all changes over time. This multi-layered approach ensures that every document in our system is authentic, current, and properly attributed to its authoritative source.
```

</details>

*[Word count: 248/250 ✅]*

---

## Data Strategy - Data Management

Our data management strategy ensures document integrity, accessibility, and preservation through multiple layers of governance and quality control. Every document entering our system undergoes verification through a transparent GitHub-based review process where human reviewers confirm accuracy of AI extraction, validate document authenticity, and ensure proper metadata attribution. This open review process creates accountability while enabling community contributions and corrections.

Version control forms the foundation of our preservation strategy. Using Git with Large File Storage, we maintain complete history of every document change, enabling researchers to access any historical version and track policy evolution over time. Documents are cryptographically hashed to detect any modifications, and our distributed architecture across multiple Git remotes prevents single points of failure. Regular backups to cold storage ensure documents remain available even in catastrophic scenarios.

Data quality metrics guide our continuous improvement process. We track extraction accuracy rates, measuring how often human reviewers must correct AI-identified documents. Response time metrics ensure our API meets performance requirements for production systems. Coverage analysis identifies gaps in our collection, prioritizing jurisdictions or document types needing attention. User feedback loops enable rapid correction of any errors that reach production.

Access control balances openness with security. Read access remains open through our public API, supporting our mission of democratizing policy information. Write access requires authentication and follows strict contribution guidelines. Rate limiting prevents abuse while ensuring legitimate users have reliable access. Usage analytics help us understand access patterns without collecting personally identifiable information about users, maintaining privacy while improving service delivery.

<details>
<summary>📋 Copy this response (250/250 words ✅)</summary>

```
Our data management strategy ensures document integrity, accessibility, and preservation through multiple layers of governance and quality control. Every document entering our system undergoes verification through a transparent GitHub-based review process where human reviewers confirm accuracy of AI extraction, validate document authenticity, and ensure proper metadata attribution. This open review process creates accountability while enabling community contributions and corrections.

Version control forms the foundation of our preservation strategy. Using Git with Large File Storage, we maintain complete history of every document change, enabling researchers to access any historical version and track policy evolution over time. Documents are cryptographically hashed to detect any modifications, and our distributed architecture across multiple Git remotes prevents single points of failure. Regular backups to cold storage ensure documents remain available even in catastrophic scenarios.

Data quality metrics guide our continuous improvement process. We track extraction accuracy rates, measuring how often human reviewers must correct AI-identified documents. Response time metrics ensure our API meets performance requirements for production systems. Coverage analysis identifies gaps in our collection, prioritizing jurisdictions or document types needing attention. User feedback loops enable rapid correction of any errors that reach production.

Access control balances openness with security. Read access remains open through our public API, supporting our mission of democratizing policy information. Write access requires authentication and follows strict contribution guidelines. Rate limiting prevents abuse while ensuring legitimate users have reliable access. Usage analytics help us understand access patterns without collecting personally identifiable information about users, maintaining privacy while improving service delivery.
```

</details>

*[Word count: 250/250 ✅]*

---

## Stakeholder Engagement

Our stakeholder engagement strategy builds on strong existing partnerships while systematically expanding to serve the entire benefits ecosystem. We secured letters of support from leading research institutions including Georgetown's Better Government Lab, Vanderbilt's Prenatal-to-3 Policy Impact Center, National Bureau of Economic Research, and Urban Institute, each committing to contribute specialized document collections and validate our approach. Direct service organizations like Benefit Kitchen, MyFriendBen, and SnapScreener confirmed the critical need for this infrastructure and committed to early adoption.

Engagement happens through multiple channels tailored to each stakeholder group. Our bounty program incentivizes experts from trusted organizations like Urban Institute and Georgia Center for Opportunity to vet AI contributions and submit their own documents, ensuring comprehensive coverage across the rules-as-code ecosystem. Direct service organizations participate in monthly user groups sharing implementation challenges. Government partners like the Atlanta Federal Reserve integrate our system into their Policy Rules Database. Community advocates contribute through our open GitHub process, ensuring affected populations have direct input.

Our partner network creates network effects that accelerate adoption. Each integrated organization becomes an advocate, sharing success stories that attract similar organizations. MyFriendBen's significant reduction in maintenance time provides compelling evidence for other direct service providers. Academic publications from university partners lend credibility that opens doors with government agencies. Technical documentation and open source code enable self-service adoption without requiring direct involvement.

Continuous feedback loops ensure we remain responsive to user needs through regular surveys, usage analytics, feature request tracking, and an active community forum where users help each other succeed.

<details>
<summary>📋 Copy this response (250/250 words ✅)</summary>

```
Our stakeholder engagement strategy builds on strong existing partnerships while systematically expanding to serve the entire benefits ecosystem. We secured letters of support from leading research institutions including Georgetown's Better Government Lab, Vanderbilt's Prenatal-to-3 Policy Impact Center, National Bureau of Economic Research, and Urban Institute, each committing to contribute specialized document collections and validate our approach. Direct service organizations like Benefit Kitchen, MyFriendBen, and SnapScreener confirmed the critical need for this infrastructure and committed to early adoption.

Engagement happens through multiple channels tailored to each stakeholder group. Our bounty program incentivizes experts from trusted organizations like Urban Institute and Georgia Center for Opportunity to vet AI contributions and submit their own documents, ensuring comprehensive coverage across the rules-as-code ecosystem. Direct service organizations participate in monthly user groups sharing implementation challenges. Government partners like the Atlanta Federal Reserve integrate our system into their Policy Rules Database. Community advocates contribute through our open GitHub process, ensuring affected populations have direct input.

Our partner network creates network effects that accelerate adoption. Each integrated organization becomes an advocate, sharing success stories that attract similar organizations. MyFriendBen's significant reduction in maintenance time provides compelling evidence for other direct service providers. Academic publications from university partners lend credibility that opens doors with government agencies. Technical documentation and open source code enable self-service adoption without requiring direct involvement.

Continuous feedback loops ensure we remain responsive to user needs through regular surveys, usage analytics, feature request tracking, and an active community forum where users help each other succeed.
```

</details>

*[Word count: 250/250 ✅]*

---

## Resources and Infrastructure

PolicyEngine Atlas leverages modern cloud infrastructure designed for scalability, reliability, and cost efficiency. Our technical stack combines proven technologies that handle millions of documents while minimizing operational complexity. Cloud hosting through AWS or Google Cloud provides auto-scaling compute resources that expand during crawling operations and contract during quiet periods, optimizing costs while ensuring performance.

Storage architecture employs a tiered approach matching access patterns to appropriate technologies. Hot storage using Git with Large File Storage maintains frequently accessed current documents with instant retrieval. Warm storage in object stores like S3 holds historical versions and less-accessed documents at lower cost. Cold storage archives ensure permanent preservation of all documents even if primary systems fail. This tiered approach reduces costs while maintaining rapid access to the documents users need most.

Our API uniquely combines document search with PolicyEngine's rules engine metadata. When searching "Lifeline eligibility," we trace encoded rules revealing NC's 200% FPL SNAP creates categorical eligibility. This knowledge graph surfaces hidden program connections impossible to find through traditional search or RAG alone, helping families discover unknown benefit eligibility.

Development infrastructure supports rapid iteration while maintaining quality. Continuous integration pipelines automatically test code changes, ensuring new features don't break existing functionality. Staging environments enable partner testing before production deployment. Monitoring systems track performance metrics, alert on anomalies, and provide insights for optimization. This robust infrastructure foundation enables us to serve hundreds of partner organizations reliably while keeping operational costs manageable.

<details>
<summary>📋 Copy this response (237/250 words ✅)</summary>

```
PolicyEngine Atlas leverages modern cloud infrastructure designed for scalability, reliability, and cost efficiency. Our technical stack combines proven technologies that handle millions of documents while minimizing operational complexity. Cloud hosting through AWS or Google Cloud provides auto-scaling compute resources that expand during crawling operations and contract during quiet periods, optimizing costs while ensuring performance.

Storage architecture employs a tiered approach matching access patterns to appropriate technologies. Hot storage using Git with Large File Storage maintains frequently accessed current documents with instant retrieval. Warm storage in object stores like S3 holds historical versions and less-accessed documents at lower cost. Cold storage archives ensure permanent preservation of all documents even if primary systems fail. This tiered approach reduces costs while maintaining rapid access to the documents users need most.

Our API uniquely combines document search with PolicyEngine's rules engine metadata. When searching "Lifeline eligibility," we trace encoded rules revealing NC's 200% FPL SNAP creates categorical eligibility. This knowledge graph surfaces hidden program connections impossible to find through traditional search or RAG alone, helping families discover unknown benefit eligibility.

Development infrastructure supports rapid iteration while maintaining quality. Continuous integration pipelines automatically test code changes, ensuring new features don't break existing functionality. Staging environments enable partner testing before production deployment. Monitoring systems track performance metrics, alert on anomalies, and provide insights for optimization. This robust infrastructure foundation enables us to serve hundreds of partner organizations reliably while keeping operational costs manageable.
```

</details>

*[Word count: 237/250 ✅]*

---

## Scalability & Sustainability

Our sustainability model transitions from grant funding to self-sufficiency through diversified revenue streams that align incentives with our public benefit mission. During the grant period, we establish the foundation for long-term viability by demonstrating clear value to multiple stakeholder groups who will sustain the system through various contribution models.

Revenue diversification ensures no single funding source controls the project's future. Premium API tiers for high-volume commercial users generate recurring revenue while keeping basic access free for nonprofits. Enterprise contracts with large organizations needing custom integrations or guaranteed support levels provide predictable income. Government contracts for specialized document collections or enhanced services leverage our expertise while serving public needs. Foundation grants for specific enhancements or expansions supplement earned revenue without creating dependency.

Cost structure improvements over time enhance sustainability. Initial AI training costs decrease as models learn document patterns, reducing per-document processing expenses. Community contributions of corrections and additions lower content acquisition costs. Automated quality checks reduce human review requirements. Economies of scale mean that doubling our document count increases costs by only thirty percent, creating improving unit economics as we grow.

Scalability mechanisms ensure growth doesn't compromise quality or accessibility. Technical architecture using serverless functions and object storage scales automatically. Content contribution protocols enable distributed collection without bottlenecks. Partnership models create self-reinforcing growth where each user organization attracts others. Geographic expansion templates allow replication in new countries with minimal investment. This combination of diversified revenue, improving economics, and scalability ensures PolicyEngine Atlas becomes permanent infrastructure for equitable benefit access.

<details>
<summary>📋 Copy this response (249/250 words ✅)</summary>

```
Our sustainability model transitions from grant funding to self-sufficiency through diversified revenue streams that align incentives with our public benefit mission. During the grant period, we establish the foundation for long-term viability by demonstrating clear value to multiple stakeholder groups who will sustain the system through various contribution models.

Revenue diversification ensures no single funding source controls the project's future. Premium API tiers for high-volume commercial users generate recurring revenue while keeping basic access free for nonprofits. Enterprise contracts with large organizations needing custom integrations or guaranteed support levels provide predictable income. Government contracts for specialized document collections or enhanced services leverage our expertise while serving public needs. Foundation grants for specific enhancements or expansions supplement earned revenue without creating dependency.

Cost structure improvements over time enhance sustainability. Initial AI training costs decrease as models learn document patterns, reducing per-document processing expenses. Community contributions of corrections and additions lower content acquisition costs. Automated quality checks reduce human review requirements. Economies of scale mean that doubling our document count increases costs by only thirty percent, creating improving unit economics as we grow.

Scalability mechanisms ensure growth doesn't compromise quality or accessibility. Technical architecture using serverless functions and object storage scales automatically. Content contribution protocols enable distributed collection without bottlenecks. Partnership models create self-reinforcing growth where each user organization attracts others. Geographic expansion templates allow replication in new countries with minimal investment. This combination of diversified revenue, improving economics, and scalability ensures PolicyEngine Atlas becomes permanent infrastructure for equitable benefit access.
```

</details>

*[Word count: 249/250 ✅]*

---

## Financial Viability

Financial viability beyond the grant period relies on proven revenue models already validated in similar contexts. Our analysis of comparable infrastructure projects shows that organizations serving policy data typically achieve break-even within 18 months of launch through a combination of subscription revenue and service contracts.

Our revenue projections build on conservative assumptions grounded in current market dynamics. With 30 partner organizations by Year 2, premium tier subscriptions at $500 monthly average generate $180,000 annually. Enterprise contracts with five large organizations at $50,000 each provide $250,000 yearly. Government contracts for specialized collections contribute another $200,000. This creates $630,000 annual revenue by grant end, covering operational costs while enabling continued growth.

Cost optimization strategies ensure financial efficiency without compromising quality. Cloud infrastructure costs decrease through reserved instance pricing. Automation reduces manual processing from 80 percent of documents in Year 1 to 20 percent by Year 2. Our bounty program incentivizes contributions from Urban Institute, Georgia Center for Opportunity, and other trusted organizations, building community ownership. Strategic partnerships share infrastructure costs.

Financial controls and governance ensure responsible stewardship. Monthly financial reviews track revenue against projections and identify variances early. Diversified revenue streams prevent over-dependence on any single source. Reserve funds equivalent to six months operating expenses provide cushion for unexpected changes. Board oversight including financial experts ensures strategic financial management. Transparent reporting to stakeholders maintains trust and accountability. This comprehensive financial strategy ensures PolicyEngine Atlas becomes self-sustaining infrastructure that continues serving vulnerable populations long after initial grant funding ends.

<details>
<summary>📋 Copy this response (246/250 words ✅)</summary>

```
Financial viability beyond the grant period relies on proven revenue models already validated in similar contexts. Our analysis of comparable infrastructure projects shows that organizations serving policy data typically achieve break-even within 18 months of launch through a combination of subscription revenue and service contracts.

Our revenue projections build on conservative assumptions grounded in current market dynamics. With 30 partner organizations by Year 2, premium tier subscriptions at $500 monthly average generate $180,000 annually. Enterprise contracts with five large organizations at $50,000 each provide $250,000 yearly. Government contracts for specialized collections contribute another $200,000. This creates $630,000 annual revenue by grant end, covering operational costs while enabling continued growth.

Cost optimization strategies ensure financial efficiency without compromising quality. Cloud infrastructure costs decrease through reserved instance pricing. Automation reduces manual processing from 80 percent of documents in Year 1 to 20 percent by Year 2. Our bounty program incentivizes contributions from Urban Institute, Georgia Center for Opportunity, and other trusted organizations, building community ownership. Strategic partnerships share infrastructure costs.

Financial controls and governance ensure responsible stewardship. Monthly financial reviews track revenue against projections and identify variances early. Diversified revenue streams prevent over-dependence on any single source. Reserve funds equivalent to six months operating expenses provide cushion for unexpected changes. Board oversight including financial experts ensures strategic financial management. Transparent reporting to stakeholders maintains trust and accountability. This comprehensive financial strategy ensures PolicyEngine Atlas becomes self-sustaining infrastructure that continues serving vulnerable populations long after initial grant funding ends.
```

</details>

*[Word count: 246/250 ✅]*

---


# Application Status Summary

- **Sections Complete:** 15/15
- **Completion Rate:** 100.0%

## Required Attachments:

- ❌ Team Member Biographies
- ❌ Project Plan/Roadmap with Milestones
- ❌ PBIF Budget Template (Completed)
- ❌ Organizational Budget 2026-2027
- ❌ Letters of Support